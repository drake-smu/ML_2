<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Hypercomplex Neural Networks</title>
    <meta charset="utf-8" />
    <meta name="author" content="Carson Drake" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="main.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Hypercomplex Neural Networks
## Quaternion Edition
### Carson Drake
### 2019/12/01 (updated: 2019-12-16)

---

class: inverse
#State of Deep Learning


## Struggles

.pull-left[



* Generalization

* Training Bias

* Over/Under fitting

* Model Size/Complexity

## Solutions?


* More complex models


* Layer Stacking


* Aggressive Data Dollection


* Adding more humans into "machine"  
learning

]

.pull-right[
![](https://imgs.xkcd.com/comics/machine_learning.png)

.footnote[image source: [xkcd.com/1838](https://xkcd.com/1838/)]
]

---
class: inverse, middle, center

# Other Options?

???
Is there anything else we can try to make 
deep learning better?  

What else can we 
besides what we've been trying?

---
class: center
# What about actual math?
.pull-left[
![](./rsc/diagrams/vectorSpace.png)

]

.pull-right[
![](./rsc/diagrams/algebras.png)
]

.footnote[
.left[
left: [www.euclideanspace.com/maths/algebra/multidimensional/vectorSpace.png](https://www.euclideanspace.com/maths/algebra/multidimensional/vectorSpace.png)  
right: ([Chappell, et al., 2016](#bib-algebra))
]]

???
Rather than trying to imagine yet another combination of layers and cells,  
lets take a step back and think about what is going on at the most fundamental 
level of these networks.   

Lets look at the math!  

Something interesting sticks out in the two graphics above.  

One subject in particular seem to be related to or fundamental for the fields 
related to mainstream algebra as well as vectors and vector space.
---
background-image:url(./rsc/memes/quaternions.jpg)
class: inverse

.footnote[
source: [https://makeameme.org/meme/quaternions]()
]

???
QUATERNIONS!

---
# Quaternion | Background
.pull-left[

`\begin{equation}
\mathbf{i}^{2}=\mathbf{j}^{2}=\mathbf{k}^{2}=\mathbf{i} \mathbf{j} \mathbf{k}=-1
\end{equation}`

`\begin{equation}
\begin{aligned} 
Q&amp;=r 1+x \mathbf{i}+y \mathbf{j}+z \mathbf{k}\\
\\
q&amp;=(r, \vec{v}), q \in \mathbf{H}, r \in \mathbf{R}, \vec{v} \in \mathbf{R}^{3}\\
\\
Q_{\operatorname{mat}}&amp;={\left[
\begin{array}{cccc}
{r} &amp; {-x} &amp; {-y} &amp; {-z} \\ 
{x} &amp; {r} &amp; {-z} &amp; {y} \\ 
{y} &amp; {z} &amp; {r} &amp; {-x} \\ 
{z} &amp; {-y} &amp; {x} &amp; {r}
\end{array}\right]}\end{aligned}
\end{equation}`


]

.pull-right[
.history[
## History

- Hamilton in 1843
- "vector" and "scalar"
- Gibbs (and vector analysis)
- Resurgence
  - Quantum Mechanics
  - Number Theory
]]

???
Sir William Hamilton
in a flash of genius discovered
the fundamental formula for
quaternion multiplication 
&amp; cut it on a stone of this bridge  

* Hamilton's work in quaternions was a polarizing topic at the time.  
Some went as far as to deam them "mixed evil".  

* Josiah Gibbs, introduced his own vector analysis. Greatly recycling the 
properties of the imaginary vector part, but rather than their components 
multiplying to -1 they equaled 0;


---
# Quaternion | Properties

.pull-left.center[
`\begin{equation}
\begin{array}{c|c c c c}
{\times}&amp;{1} &amp; {i} &amp; {j} &amp; {k}\\ 
\hline {1} &amp; {1} &amp; {i} &amp; {j} &amp; {k}\\
{i} &amp; {i} &amp; {-1} &amp; {k} &amp; {-j}\\ 
{j} &amp; {j} &amp; {-k} &amp; {-1} &amp; {i}\\ 
{k} &amp; {k} &amp; {j} &amp; {-i} &amp; {-1}\end{array}
\end{equation}`

&lt;br/&gt;
![:scale 200px](./rsc/diagrams/i_to_j_to_k.svg)
]

.pull-right.history[
### Properties

* Non-commuting
* Anti-commuting
* Double Cover

`\begin{equation}
\begin{aligned} 
i j &amp;=-j i=k \\ 
j k &amp;=-k j=i \\ 
k i &amp;=-i k=j \end{aligned}
\end{equation}`

.footnote[
.right[
&lt;!--top: [By Maschen - Own work, CC BY-SA 3.0](https://commons.wikimedia.org/w/index.php?curid=27793856--&gt;
image source: [By Cayley_graph_Q8.png: Sullivan.t.j.Original uploader was Sullivan.t.j at en.wikipediaderivative CC BY-SA 3.0](https://commons.wikimedia.org/w/index.php?curid=16302914)
]]
]

???
* The "redundancy" Gibbs did away with is in fact the part of quaternions 
that make them appealing today.
* Non commuting means that the order operating is important. This makes them
very useful for calculating spacial position and roations in 3-D. Try putting
you phone down on a surface.
Flip it over (away from you)  
rotate it 90 degrees.  
Note the position.  
Now begin in the same start position but this time rotate 90, then flip over its end.  
You should find it face down but oriented the opposite direction.
* This is related to double cover. In 3D space, a quaternion performs 2 full rotations 
to complete 1 full 4d rotation.


---
class: inverse, middle, center
# Quaternion Valued &lt;br/&gt;Neural Networks

---
# QNN Concepts

.pull-left.biggerer.blue[
  
  ## We will cover...
  * Hamilton Product
  * Activation Functions
  * Loss functions

]

.pull-right.biggerer[ 
  .red[
  ## We will not cover...
  * Batch Normalization
  * Weight Initialization
  * Backpropagation and Chain Rule  
  ]
  
]
&lt;br&gt;&lt;br&gt;

.biggerer[
###.red[**"Will not cover"**] `\(\bf\neq\)` **"Not important"**
 **These "Will Not Cover" concepts are actually quite important.  
 *HIGHLY RECOMMEND READING* SECTIONS DESCRIBING THESE CONCEPTS IN  
 ([Gaudet, et al., 2018](#bib-gaudet_maida_2018)) and ([Parcollet, et al., 2019](#bib-qsurvey))**
]

???
@TODO
 however given that slide presentations are ill suited for conveying extensive mathematical 
 derivatives and nuances of group theory, I am going to point 

---
#Hamilton Product

![](./rsc/diagrams/hamilton.png)

.center[
`\begin{equation}
\begin{aligned}\
Q_{1}\otimes Q_{2} &amp;\neq Q_{2} \otimes Q_{1} \\
Q_{1}\otimes Q_{2} &amp;=\left(r_{1} r_{2}-x_{1} x_{2}-y_{1} y_{2}-z_{1} z_{2}\right) \\ 
&amp;+\left(r_{1} x_{2}+x_{1} r_{2}+y_{1} z_{2}-z_{1} y_{2}\right) i \\ 
&amp;+\left(r_{1} y_{2}-x_{1} z_{2}+y_{1} r_{2}+z_{1} x_{2}\right) j \\ 
&amp;+\left(r_{1} z_{2}+x_{1} y_{2}-y_{1} x_{2}+z_{1} r_{2}\right) k \end{aligned}
\end{equation}`
]

.footnote[
source: ([Parcollet, et al., 2019](#bib-qConv))
]

???
* The Hamilton product (⊗) is used in QNN to remplace the standard real-valued dot
product, and to perform transforma- tions between two quaternions

* In the case of a quaternion-valued neural network, the quaternion-weight
components are shared through multiple quaternion-input parts during the
Hamilton product , creating relations within the elements.

---
#  Activation Function

.pull-left[
## Quaternion Functions

* Promising Results
* Singularities
* Complicate Training

`\begin{equation}
\small
\operatorname{sig}(Q)=\frac{1}{1+e^{-Q}}, \quad \tanh (Q)=\frac{e^{2 Q}-1}{e^{2 Q}+1}\\
\small
e^{Q}=e^{s}\left(\cos |\vec{v}|+\frac{\vec{v}}{|\vec{v}|} \sin |\vec{v}|\right)
\end{equation}`
]

.pull-right[
## Split Activation

* Component wise activation
* Simplify Training
* Real-value Functions (tanh, sigmoid, etc)


`\begin{equation}
\small
\alpha(Q)=f(r)+f(x) \mathbf{i}+f(y) \mathbf{j}+f(z) \mathbf{k}
\end{equation}`
]
???
There are fully quaternion activation functions, such the hypercomplex sigmoid
or the hyperbolic tangent. Though these quaternion-valued activation functions
show promising results, they require careful training due to an important number
of singularities that can drastically alter the QNN performances.

* The more common, solution is the split activation function. Split Activstion
 functions apply real-valued activation functions componentwise. 
 
* Split activation functions map quaternion numbers back to the real-valued
space by ignoring the nature of the relation that exists between the components
* Multi-view learning ability of the QNN is reduced by the fact that this
relation is captured by the Hamilton product

---
# Loss Function (Back propagation)

.pull-left[
## Split Loss Function
* Similar to Split Activation
* Quaternion Mean Square Error
]

.pull-right[
## Compromise
* Hybrid Model
* QLoss Function Unsolved
* Probability distribution issue
]

`\begin{equation}
\small
E=\frac{1}{N} \sum_{n=1}^{N}\left[\left(t_{r p n}-S_{r n}^{M}\right)^{2}+\left(t_{i p n}-S_{i n}^{M}\right)^{2}+\left(t_{j p n}-S_{j n}^{M}\right)^{2}+\left(t_{k p n}-S_{k n}^{M}\right)^{2}\right]
\end{equation}`
`\begin{equation}
\small
\Delta^{l}=\frac{\partial E}{\partial W^{l}}=\frac{\partial E}{\partial W_{r}^{l}}+\mathrm{i} \frac{\partial E}{\partial W_{i}^{l}}+\mathbf{j} \frac{\partial E}{\partial W_{j}^{l}}+\mathbf{k} \frac{\partial E}{\partial W_{k}^{l}}
\end{equation}`

???
* The definition of quaternion loss functions mostly remain an unsolved problem
* Quaternion probability distribution is not trivial, this commonly results in the 
usage of a real-valued final layer for learning classification tasks.
* An extension of the mean squared error to quaternions for approximation tasks 
was introduced by solely replacing real-numbers with hyper-complex numbers
* The QMSE is a split loss function based on the standard MSE. 
* The lack of cost functions based purely on quaternion algebra technically makes
quaternion neural networks hybrid models.

---
background-image: url(./rsc/real_conv2.png)
background-size: contain
# CNN vs QCNN (Real)


.footnote[
source: ([Zhu, et al., 2018](#bib-qConv2))
]

???

* In regards to image recognition, a good model has to efficiently
encode local relations within the input features, such as between the Red,
Green, and Blue (R,G,B) channels of a single pixel, as well as structural
relations  

* Traditional real-valued CNNs consider pixels as three different and separated
values (R, G, B)

---
background-image: url(./rsc/q_conv2.png)
background-size: contain
# CNN vs QCNN (Quaternion)



.footnote[
source: ([Zhu, et al., 2018](#bib-qConv2))
]

???
*  a more natural representation is to process a pixel as a
single multi- dimensional entity

---
background-image: url(/rsc/diagrams/conv_q.png)
background-size: contain
# CNN vs QCNN (Hamilton Product)



.footnote[
source: ([Gaudet, et al., 2018](#bib-gaudet_maida_2018))
]

???
This diagram from Gaudet, et al's paper, exploring the same concept of QCNN's, 
helps illustrate the weight to input interaction that occurs with the Hamilton 
Product.  

As we can see, the rijk components interact across all input components. 
This is where the local intra pixel color to color relations are learned.  

This cross component interaction is not present in typical CNN's

---
# LSTM vs QLSTM


.pull-left[

`\begin{equation}
\begin{aligned}\
&amp;\text{Quaternion-Valued}\\
f_{t}&amp;=\sigma\left(W_{f} \otimes x_{t}+R_{f} \otimes h_{t-1}+b_{f}\right)\\
i_{t}&amp;=\sigma\left(W_{i} \otimes x_{t}+R_{i} \otimes h_{t-1}+b_{i}\right)\\ 
o_{t}&amp;=\sigma\left(W_{o} \otimes x_{t}+R_{o} \otimes h_{t-1}+b_{o}\right)\\  
c_{t}&amp;=f_{t} \times c_{t-1}\\
&amp;{ }+ i_{t} \times \alpha\left(W_{c} \otimes x_{t}+R_{c} \otimes h_{t-1}+b_{c}\right)\\
h_{t}&amp;=o_{t} \times \alpha\left( c_{t} \right)\end{aligned}
\end{equation}`
]

.pull-right[
`\begin{equation}
\begin{aligned}\
&amp;\text{Real-Valued}\\
f_{t} &amp;=\sigma_{g}\left(W_{f} x_{t}+U_{f} h_{t-1}+b_{f}\right)\\
i_{t} &amp;=\sigma_{g}\left(W_{i} x_{t}+U_{i} h_{t-1}+b_{i}\right) \\
o_{t} &amp;=\sigma_{g}\left(W_{o} x_{t}+U_{o} h_{t-1}+b_{o}\right) \\
c_{t} &amp;=f_{t} \circ c_{t-1}\\
&amp;+i_{t} \circ \sigma_{c}\left(W_{c} x_{t}+U_{c} h_{t-1}+b_{c}\right) \\ 
h_{t} &amp;=o_{t} \circ \sigma_{h}\left(c_{t}\right)\end{aligned}
\end{equation}`

]

.center[
![:scale 50%](./rsc/diagrams/lstm.png)
]

.caption.center[LSTM cell]

.footnote[
source: ([Rathor, 2019](#bib-rathor))
]

???

QRNNs are expected to code both sequential dependencies due to the recurrent
architecture and internal relations between multi-dimensional input features
that compose the sequence due to the Hamilton product.

Comparing the set of equations used in the respective networks we can see that 
the underlying structure is largely identical. The main exception being in the 
QLSTM's use of split functions (represented by alpha), which we previously discussed.  

---
class: inverse, middle, center
# QNN Research&lt;br/&gt;Examples


---
# Speech Recognition

![:scale 100%](./rsc/diagrams/qlstm_res.png)

.caption.center[Phoneme Error Rate (PER)%]

.footnote[
source: ([Parcollet, et al., 2019](#bib-parcollet_morchid_linares_mori_2019))
]

???
The first QNN examples I want to walk through is the QLSTM and QRNN. The raw
audio is first transformed into 40-dimensional log Mel-filter-bank coefficients.  


Q(f, t) representsan acoustic quaternion multiple views of a frequency f at time
frame t, consisting of the energy e(f, t) in the filter band at frequency f, its
first time derivative describing a slope view, its second time derivative
describing a concavity view, and the third derivative describing the rate of
change of the second derivative

The reported results support the initial intuitions by showing that QLSTM are
more effective at learning both longer dependencies and a compact representation
of multidimensional acoustic speech features by outperforming standard
real-valued LSTMs on both experiments, with up to 2.8 times less neural
parameters

---
# Speech Recognition Pt. 2

.center[

![:scale 75%](./rsc/diagrams/qlstm_res2.png)
]

.caption.center[
Cross entropy loss, and accuracy  
QLSTM (Blue) and LSTM (Orange)
]


.footnote[

source: ([Parcollet, et al., 2019](#bib-parcollet_morchid_linares_mori_2019))
]

???
To verify that the quaternion algebra was not imparing the network functionality
of an LSTM, Parcollet performed a synthetic memory copy-task for time lags of
blanks T of 10, 50, and 100. From the Evolution of the cross entropy loss, and
of the accuracy of QLSTM (Blue curves) and LSTM (Orange curves) we see that not
only does the QLSTM converge faster than the real-value counter part, it also
manages to converge at 100 lag where the real value LSTM fails.

---
# Heterogeneous Image Processing

## Oh my generalization!

![:scale 100%](./rsc/diagrams/results_QCAE.png)
.footnote[
source: ([Parcollet, et al., 2019](#bib-qConv))
]

???
The small number of neurons allows the QCAE to ob- tain “robust” and “compact”
memory that build a robust hid- den representations of the image content in the
latent space. Indeed, QCAE are not altered by heterogeneous color spaces  

They demonstrate that quaternion convolutional encoder-decoders are able to
perfectly learn the color-space with a training performed on a unique gray-scale
image,  

while real-valued CAE fail,  

The Hamilton product allows QNN to encode local dependencies such as the RGB 
relation of a pixel.  

QCAE produced better quality reconstructions with respect to the PSNR and SSIM metrics
than CAE, even with considering the unability of CAE to learn colors

---
# Replicated QCAE


.pull-left.small.demo[
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;**QCAE**&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; **CAE**  
![:scale 30%](./rsc/save_image_QCAE_0.png) 
![:scale 30%](./rsc/save_image_CAE_0.png) 

![:scale 30%](./rsc/save_image_QCAE_100.png)
![:scale 30%](./rsc/save_image_CAE_100.png)  

![:scale 30%](./rsc/save_image_QCAE_3000.png)
![:scale 30%](./rsc/save_image_CAE_3000.png)
]

.pull-right.history[
## Results
* Confirmation QCAE learned full color space
* CAE failed to learn colors
* More accurate overall reconstruction

## Implications
* Limited training data
* Better generalization
* Help counter biases
]

???
Given the findings described in the heterogeneous autoencoder paper, I had to
see it to believe it. Fortunately Parcollet's code is publically avaibale. After
forking and following the brief setup I was able to accurately replicate the
findings described in the paper. Full disclosure I haven't fully verified the
full quaternion implementation used, but I did alter the images used for
training, testing, and verification and found that the model performed
consistently.

I hope to dig deeper into the code implementation, and possiby evaluate different 
classification models utilizing the QCAE embedding with biased training data.

---
# R2H (Real to Quaternion) Layer


![:scale 100%](./rsc/diagrams/r2h.png)
.footnote[
source: ([Parcollet, et al., 2019](#bib-r2h))
]

???
R2H is a very important step towards seamlessly integrating hypercomplex
networks into hybrid real-quaternion valued models. One of the biggest hurtles
for implementing QNN's is the added work needed to transform real value data to
quaternion inputs.

There are various combinations that can be made for different dimentional data.
For instance it may be better to use two input quaternions with one consisting
of ijk components and the other of only r. A conversion layer such as R2H can
establish the optimal representation automatically.


---
class: inverse, middle, center
# Closing...


---
# Closing...

.pull-left.bigger[
## Recap
* Robust Compact Representations
* Consistently outperform equivalent Real-Valued NN
* Core basic NN functions have been implemented. More research needed
* Learn local and global relationships in higher dimentional data
]

.pull-right.bigger[
## Going Forward
* Quaternion-valued Features
* New Learning Algorithms
* New Architectures
* Proper CUDA Implementation
]

???
&lt;!--Recap--&gt;
* The specific four dimensional algebra of quaternion numbers, including the
Hamilton product allows quaternion-valued models to consistently outperform
equivalent real-valued neural networks

* QNNs suffer from the fact of being a resurgent field

&lt;!--Going Forward--&gt;
* Currently, most features used in quaternion-valued models are straightforward 
or commons real-valued features. New data preprocessing methods have to be
investigated to naturally project the features into the quaternion space, such
as the quaternion Fourier transform

* Current QNN architectures are based on the straight forward extension of the
real-valued backpropagation to the quaternion domain. `\(G\mathbb{HR}\)` calculus
makes it possible to propose well adapted learning algorithms that can speed-up
the training, and increase the performances due to a better consideration of the
quaternion algebra.

* New neural networks architectures,such as capsule networks, or generative
adversarial neural networks which could benefits from the introduction of
quaternion numbers, are still missing.

* The Hamilton Product is a computationally expensive operation. Thought current
model sizes of QNN are more compact than their real-valued equivalent, their
training times are distinctly worse.

---
# Credit : Parcollet, et al

## Recent Papers

* Real to H-Space Encoder for Speech Recognition [https://arxiv.org/pdf/1906.08043.pdf]()
* Quaternion Convolutional Neural Networds for Heterogeneus Image Processing 
[https://arxiv.org/pdf/1811.02656.pdf]()

## Other

* Github Repo [Orkis-Research](https://github.com/Orkis-Research)
* A survey of quaternion neural networks [Artificial Intelligence Review](https://doi.org/10.1007/s10462-019-09752-1)

.footnote[
Parcollet's website: [http://darnault-parcollet.fr/Parcollet]()
]

???
Parcollet dedicated his PhD thesis to the investigation and development of  
hypercomplex neural networks.   

As much as I'd like to walk you through all of his related works, I'm afraid we 
only have time to highlight some of them. Below is a link to his personal site where 
you can find all of his papers and ongoing research.

---
# References

&lt;a name=bib-algebra&gt;&lt;/a&gt;[Chappell, J. M. et al.](#cite-algebra) "The Vector Algebra War: A Historical Perspective". In: _IEEE
Access_ 4 (2016), p. 1997–2004. DOI: [10.1109/access.2016.2538262](https://doi.org/10.1109%2Faccess.2016.2538262).

&lt;a name=bib-gaudet_maida_2018&gt;&lt;/a&gt;[Gaudet, C. J. et al.](#cite-gaudet_maida_2018) "Deep Quaternion Networks". In: _2018
International Joint Conference on Neural Networks (IJCNN)_ (2018). DOI:
[10.1109/ijcnn.2018.8489651](https://doi.org/10.1109%2Fijcnn.2018.8489651).

&lt;a name=bib-qConv&gt;&lt;/a&gt;[Parcollet, T. et al.](#cite-qConv) "Quaternion Convolutional Neural Networks for Heterogeneous Image
Processing". In: _ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_ (2019).
DOI: [10.1109/icassp.2019.8682495](https://doi.org/10.1109%2Ficassp.2019.8682495).

&lt;a name=bib-qsurvey&gt;&lt;/a&gt;[Parcollet, T. et al.](#cite-qsurvey) "A survey of quaternion neural networks". In: _Artificial
Intelligence Review_ (2019). DOI: [10.1007/s10462-019-09752-1](https://doi.org/10.1007%2Fs10462-019-09752-1).

&lt;a name=bib-parcollet_morchid_linares_mori_2019&gt;&lt;/a&gt;[Parcollet, T. et al.](#cite-parcollet_morchid_linares_mori_2019)
"Bidirectional Quaternion Long Short-term Memory Recurrent Neural Networks for Speech Recognition". In: _ICASSP 2019 - 2019 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP)_ (2019). DOI:
[10.1109/icassp.2019.8683583](https://doi.org/10.1109%2Ficassp.2019.8683583).


---
# References II
&lt;a name=bib-r2h&gt;&lt;/a&gt;[Parcollet, T. et al.](#cite-r2h) "Real to H-Space Encoder for Speech Recognition". In: _Interspeech 2019_
(2019). DOI: [10.21437/interspeech.2019-1539](https://doi.org/10.21437%2Finterspeech.2019-1539).

&lt;a name=bib-rathor&gt;&lt;/a&gt;[Rathor, S.](#cite-rathor) _https://medium.com/@saurabh.rathor092/_. 2019. URL:
[https://miro.medium.com/max/862/1*GSZ0ZQZPvcWmTVatAeOiIw.png](https://miro.medium.com/max/862/1*GSZ0ZQZPvcWmTVatAeOiIw.png).

&lt;a name=bib-qConv2&gt;&lt;/a&gt;[Zhu, X. et al.](#cite-qConv2) "Quaternion Convolutional Neural Networks". In: _Computer Vision – ECCV
2018 Lecture Notes in Computer Science_ (2018), p. 645–661. DOI:
[10.1007/978-3-030-01237-3_39](https://doi.org/10.1007%2F978-3-030-01237-3_39).
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="libs/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
