---
title: "Complex and Hypercomplex Neural Networks"
subtitle: "ft. Quaternions and friends"
author: "Carson Drake"
institute: "SMU DS7335"
date: "2019/12/01 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    chakra: libs/remark-latest.min.js
    lib_dir: libs
    nature:
      beforeInit: "macros.js"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

background-image: url("rsc/memes/confused.jpg")
background-size: contain
class: center, bottom, inverse






???

Image Credit : https://news.sky.com/story/blinking-white-guy-meet-the-man-using-meme-fame-to-raise-thousands-for-charity-11819235

---
class: inverse, middle, center

# What??

???
* Explain what Quaternions are and how they relate to complex hyper complex

* Imaginary

---

#Why



![:scale 50%](rsc/test_out_QCAE.png)

---


# Training Data

![:scale 60%](rsc/save_image_gray_training.png)

---

# Conv Auto Encoder
![:scale 30%](rsc/save_image_CAE_0.png)
![:scale 30%](rsc/save_image_CAE_100.png)
![:scale 30%](rsc/save_image_CAE_3000.png)


---
# Quaternion Conv Auto Encoder
![:scale 30%](rsc/save_image_QCAE_0.png)
![:scale 30%](rsc/save_image_QCAE_100.png)
![:scale 30%](rsc/save_image_QCAE_3000.png)

--

###...ok but why? 
### Lets look into the math going on here.Its got to be obvious. 

---
## What we know: 
* Architecture of the Quaternion networks are not unfamiliar
  * Q flavored LSTM, CNN, AutoEncoder etc.
  * Trade out Real Number math for Hamilton $\mathbb{R} \rightarrow \mathbb{H}$

---
class: inverse, center

#[BEGIN: Descent to Madness]
![](rsc/memes/charlie.gif)

---






